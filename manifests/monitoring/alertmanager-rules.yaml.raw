apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: monitoring
spec:
  groups:
    - name: node-exporter.rules
      rules:
        - expr: |
            count without (cpu) (
              count without (mode) (
                node_cpu_seconds_total{job="node-exporter"}
              )
            )
          record: instance:node_num_cpu:sum
        - expr: |
            1 - avg without (cpu, mode) (
              rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
            )
          record: instance:node_cpu_utilisation:rate1m
        - expr: |
            (
              node_load1{job="node-exporter"}
            /
              instance:node_num_cpu:sum{job="node-exporter"}
            )
          record: instance:node_load1_per_cpu:ratio
        - expr: |
            1 - (
              node_memory_MemAvailable_bytes{job="node-exporter"}
            /
              node_memory_MemTotal_bytes{job="node-exporter"}
            )
          record: instance:node_memory_utilisation:ratio
        - expr: |
            rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
          record: instance:node_vmstat_pgmajfault:rate1m
        - expr: |
            rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
          record: instance_device:node_disk_io_time_seconds:rate1m
        - expr: |
            rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
          record: instance_device:node_disk_io_time_weighted_seconds:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_receive_bytes_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_transmit_bytes_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_receive_drop_excluding_lo:rate1m
        - expr: |
            sum without (device) (
              rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
            )
          record: instance:node_network_transmit_drop_excluding_lo:rate1m
    - name: k8s.rules
      rules:
        - expr: |
            sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])) by (namespace)
          record: namespace:container_cpu_usage_seconds_total:sum_rate
        - expr: |
            ( sum by (container, pod, namespace) (
              rate(container_cpu_usage_seconds_total{container!=""}[30m])
            ) ) / ( sum by (container, pod, namespace) (kube_pod_container_resource_requests_cpu_cores) )*100
          record: namespace_pod_container:container_cpu_usage_percentage:sum_rate
        - expr: |
            sum by (namespace, pod, container) (
              rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])
            ) * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
        - expr: |
            container_memory_working_set_bytes{job="kubelet", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_working_set_bytes
        - expr: |
            container_memory_rss{job="kubelet", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_rss
        - expr: |
            container_memory_cache{job="kubelet", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_cache
        - expr: |
            container_memory_swap{job="kubelet", image!=""}
            * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
          record: node_namespace_pod_container:container_memory_swap
        - expr: |
            sum(container_memory_usage_bytes{job="kubelet", image!="", container!="POD"}) by (namespace)
          record: namespace:container_memory_usage_bytes:sum
        - expr: |
            sum by (namespace) (
                sum by (namespace, pod) (
                    max by (namespace, pod, container) (
                        kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"}
                    ) * on(namespace, pod) group_left() max by (namespace, pod) (
                        kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
        - expr: |
            sum by (namespace) (
                sum by (namespace, pod) (
                    max by (namespace, pod, container) (
                        kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"}
                    ) * on(namespace, pod) group_left() max by (namespace, pod) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                    )
                )
            )
          record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
        - expr: |
            sum(
              label_replace(
                label_replace(
                  kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                  "replicaset", "$1", "owner_name", "(.*)"
                ) * on(replicaset, namespace) group_left(owner_name) kube_replicaset_owner{job="kube-state-metrics"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (namespace, workload, pod)
          labels:
            workload_type: deployment
          record: mixin_pod_workload
        - expr: |
            sum(
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (namespace, workload, pod)
          labels:
            workload_type: daemonset
          record: mixin_pod_workload
        - expr: |
            sum(
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
                "workload", "$1", "owner_name", "(.*)"
              )
            ) by (namespace, workload, pod)
          labels:
            workload_type: statefulset
          record: mixin_pod_workload
    - name: node.rules
      rules:
        - expr: sum(min(kube_pod_info) by (node))
          record: ":kube_pod_info_node_count:"
        - expr: |
            max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
          record: "node_namespace_pod:kube_pod_info:"
        - expr: |
            count by (node) (sum by (node, cpu) (
              node_cpu_seconds_total{job="node-exporter"}
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            ))
          record: node:node_num_cpu:sum
        - expr: |
            sum(
              node_memory_MemAvailable_bytes{job="node-exporter"} or
              (
                node_memory_Buffers_bytes{job="node-exporter"} +
                node_memory_Cached_bytes{job="node-exporter"} +
                node_memory_MemFree_bytes{job="node-exporter"} +
                node_memory_Slab_bytes{job="node-exporter"}
              )
            )
          record: :node_memory_MemAvailable_bytes:sum
    - name: kube-prometheus-node-recording.rules
      rules:
        - expr:
            sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY
            (instance)
          record: instance:node_cpu:rate:sum
        - expr:
            sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}))
            BY (instance)
          record: instance:node_filesystem_usage:sum
        - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
          record: instance:node_network_receive_bytes:rate:sum
        - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
          record: instance:node_network_transmit_bytes:rate:sum
        - expr:
            sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
            (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
            BY (instance, cpu)) BY (instance)
          record: instance:node_cpu:ratio
        - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
          record: cluster:node_cpu:sum_rate5m
        - expr:
            cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
            BY (instance, cpu))
          record: cluster:node_cpu:ratio
    - name: log-count-record
      interval: 1d
      rules:
        - record: average:elastic_documents_by_namespace_cluster_node:1d
          expr: |
            sum_over_time(elastic_documents_by_namespace_cluster_node[1d]) / count_over_time(elastic_documents_by_namespace_cluster_node[1d])
    - name: log-count-alarm
      rules:
        - alert: LogVolumeHigh
          annotations:
            description: |
              Namespace {{ $labels.count_namespace }} on node {{ $labels.count_node }} generating logs at double the weekly average rate:
              {{ printf "elastic_documents_by_namespace_cluster_node{count_namespace='%s', count_node='%s'} Labels.count_namespace Labels.count_node" | query | first | value }} > {{ printf "sum_over_time(average:elastic_documents_by_namespace_cluster_node:1d{count_namespace='%s', count_node='%s'}[7d])/count_over_time(average:elastic_documents_by_namespace_cluster_node:1d{count_namespace='%s', count_node='%s'}[7d]) Labels.count_namespace Labels.count_node Labels.count_namespace Labels.count_node" | query | first | value }}
          expr: |
            elastic_documents_by_namespace_cluster_node > 2 * (sum_over_time(average:elastic_documents_by_namespace_cluster_node:1d[7d])/count_over_time(average:elastic_documents_by_namespace_cluster_node:1d[7d]))
          for: 10m
          labels:
            severity: warning
    - name: kubernetes-resources
      rules:
        - alert: NodeCPUOverSubscribedWarning
          annotations:
            description: Node {{$labels.node}} CPU limits oversubscribed more than 2x
          expr: |
            sum by (node) (kube_pod_container_resource_limits_cpu_cores{node!=""} ) / on(node) group_left() kube_node_status_allocatable_cpu_cores > 2 and sum(label_replace((instance:node_cpu_utilisation:rate1m{instance!=""}), "node", "$1", "instance", "(.+)")) by (node) > 0.6
          for: 1m
          labels:
            severity: warning
        - alert: NodeCPUOverSubscribedAlarm
          annotations:
            description: Node {{$labels.node}} CPU limits oversubscribed more than 3x
          expr: |
            sum by (node) (kube_pod_container_resource_limits_cpu_cores{node!=""} ) / on(node) group_left() kube_node_status_allocatable_cpu_cores > 3 and sum(label_replace((instance:node_cpu_utilisation:rate1m{instance!=""}), "node", "$1", "instance", "(.+)")) by (node) > 0.6
          for: 1m
          labels:
            severity: critical
        - alert: NodeMemOverSubscribedWarning
          annotations:
            description: Node {{$labels.node}} Memory limits oversubscribed more than 2x
          expr: |
            sum by (node) (kube_pod_container_resource_limits_memory_bytes{node!=""} ) / on(node) group_left() kube_node_status_allocatable_memory_bytes > 2 and sum(label_replace((instance:node_memory_utilisation:ratio{instance!=""}), "node", "$1", "instance", "(.+)")) by (node) > 0.6
          for: 1m
          labels:
            severity: warning
        - alert: NodeMemOverSubscribedAlarm
          annotations:
            description: Node {{$labels.node}} Memory limits oversubscribed more than 3x
          expr: |
            sum by (node) (kube_pod_container_resource_limits_memory_bytes{node!=""} ) / on(node) group_left() kube_node_status_allocatable_memory_bytes > 3 and sum(label_replace((instance:node_memory_utilisation:ratio{instance!=""}), "node", "$1", "instance", "(.+)")) by (node) > 0.6
          for: 1m
          labels:
            severity: critical
        - alert: NodeCPUUsage
          annotations:
            description: Node {{$labels.instance}} CPU usage exceeds 85%
          expr: |
            instance:node_cpu:ratio > 0.85
          labels:
            severity: warning
        - alert: NodeMemUsage
          annotations:
            description: Node {{$labels.instance}} Memory usage exceeds 85%
          expr: |
            instance:node_memory_utilisation:ratio > 0.85
          labels:
            severity: warning
        - alert: KubeCPUOvercommit
          annotations:
            message:
              Cluster has overcommitted CPU resource requests for Pods and cannot
              tolerate node failure.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
          expr: |
            sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
              /
            sum(kube_node_status_allocatable_cpu_cores)
              >
            (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
          for: 5m
          labels:
            severity: warning
        - alert: KubeMemOvercommit
          annotations:
            message:
              Cluster has overcommitted memory resource requests for Pods and cannot
              tolerate node failure.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
          expr: |
            sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
              /
            sum(kube_node_status_allocatable_memory_bytes)
              >
            (count(kube_node_status_allocatable_memory_bytes)-1)
              /
            count(kube_node_status_allocatable_memory_bytes)
          for: 5m
          labels:
            severity: warning
        - alert: KubeCPUOvercommit
          annotations:
            message: Cluster has overcommitted CPU resource requests for Namespaces.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
          expr: |
            sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
              /
            sum(kube_node_status_allocatable_cpu_cores)
              > 1.5
          for: 5m
          labels:
            severity: warning
        - alert: KubeMemOvercommit
          annotations:
            message: Cluster has overcommitted memory resource requests for Namespaces.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
          expr: |
            sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
              /
            sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
              > 1.5
          for: 5m
          labels:
            severity: warning
        - alert: KubeQuotaExceeded
          annotations:
            message:
              Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
              }} of its {{ $labels.resource }} quota.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
          expr: |
            kube_resourcequota{job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
              > 0.95
          for: 15m
          labels:
            severity: info
    - name: kubernetes-storage
      rules:
        - alert: KubePersistentVolumeUsageCritical
          annotations:
            message:
              The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
              }} free.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
          expr: |
            kubelet_volume_stats_available_bytes{job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet"}
              < 0.03
          for: 1m
          labels:
            severity: critical
        - alert: KubePersistentVolumeUsage
          annotations:
            message:
              The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
              }} free.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
          expr: |
            kubelet_volume_stats_available_bytes{job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet"}
              < 0.20
          for: 1m
          labels:
            severity: warning
        - alert: KubePersistentVolumeFullInFourDays
          annotations:
            message:
              Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is expected to fill up within four
              days. Currently {{ $value | humanizePercentage }} is available.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
          expr: |
            (
              kubelet_volume_stats_available_bytes{job="kubelet"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubelet"}
            ) < 0.15
            and
            predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
          for: 1h
          labels:
            severity: critical
        - alert: KubePersistentVolumeErrors
          annotations:
            message:
              The persistent volume {{ $labels.persistentvolume }} has status {{
              $labels.phase }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
          expr: |
            kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
          for: 5m
          labels:
            severity: critical
    - name: kubernetes-system
      rules:
        - alert: KubeVersionMismatch
          annotations:
            message:
              There are {{ $value }} different semantic versions of Kubernetes
              components running.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
          expr: |
            count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
          for: 15m
          labels:
            severity: warning
        - alert: KubeClientErrors
          annotations:
            message:
              Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
              }}' is experiencing {{ $value | humanizePercentage }} errors.'
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
          expr: |
            (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
              /
            sum(rate(rest_client_requests_total[5m])) by (instance, job))
            > 0.01
          for: 15m
          labels:
            severity: warning
    - name: PodResources
      rules:
        - alert: ExcessivePodCPURatio
          annotations:
            description: Container {{$labels.namespace}}/{{$labels.pod}}:{{$labels.container}} has a CPU request to limit ratio greater than 2
          expr: |
            kube_pod_container_resource_limits_cpu_cores{node!=""}  > 1 and kube_pod_container_resource_limits_cpu_cores{node!=""} > 2*kube_pod_container_resource_requests_cpu_cores{node!=""}
          for: 1m
          labels:
            severity: warning
    - name: pod-resource-allocation
      rules:
        - alert: ContainerCPUOverProvisioned
          annotations:
            message: Container {{ $labels.namespace }} / {{ $labels.pod }} / {{ $labels.container }} is using less than 10 percent of allocated CPU
          expr: |
            namespace_pod_container:container_cpu_usage_percentage:sum_rate < 10 and
            (sum by (container, pod, namespace) (kube_pod_container_resource_requests_cpu_cores{namespace!="postgres-operator"})) > 1
          for: 10m
          labels:
            severity: info
        - alert: ContainerRequestTooLarge
          annotations:
            message: Container {{ $labels.container }} in Pod {{ $labels.pod }} is requesting more than 50% of {{ $labels.node }}
          expr: |
            (kube_pod_container_resource_requests_cpu_cores{node!=""}) / on(node) group_left() node:node_num_cpu:sum >= 0.5
          for: 5m
          labels:
            severity: warning
    - name: sealed-secrets
      rules:
        - alert: SecretUnsealError
          annotations:
            message: A sealed secret in {{ $labels.namespace }} namespace cannot unseal correctly
          expr: |
            increase(sealed_secrets_controller_unseal_errors_total[1d]) > 1
          for: 1m
          labels:
            severity: warning
    - name: kubernetes-apps
      rules:
        - alert: KubePodCrashLooping
          annotations:
            description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
            summary: Pod is crash looping.
          expr: |
            max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", job="kube-state-metrics"}[5m]) >= 1
          for: 15m
          labels:
            severity: warning
        - alert: KubePodNotReady
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
            summary: Pod has been in a non-ready state for more than 15 minutes.
          expr: |
            sum by (namespace, pod) (
              max by(namespace, pod) (
                kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}
              ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (
                1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
              )
            ) > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeDeploymentGenerationMismatch
          annotations:
            description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
            summary: Deployment generation mismatch due to possible roll-back
          expr: |
            kube_deployment_status_observed_generation{job="kube-state-metrics"}
              !=
            kube_deployment_metadata_generation{job="kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
        - alert: KubeDeploymentReplicasMismatch
          annotations:
            description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
            summary: Deployment has not matched the expected number of replicas.
          expr: |
            (
              kube_deployment_spec_replicas{job="kube-state-metrics"}
                >
              kube_deployment_status_replicas_available{job="kube-state-metrics"}
            ) and (
              changes(kube_deployment_status_replicas_updated{job="kube-state-metrics"}[10m])
                ==
              0
            )
          for: 15m
          labels:
            severity: warning
        - alert: KubeStatefulSetReplicasMismatch
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
            summary: Deployment has not matched the expected number of replicas.
          expr: |
            (
              kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas{job="kube-state-metrics"}
            ) and (
              changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[10m])
                ==
              0
            )
          for: 15m
          labels:
            severity: warning
        - alert: KubeStatefulSetGenerationMismatch
          annotations:
            description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
            summary: StatefulSet generation mismatch due to possible roll-back
          expr: |
            kube_statefulset_status_observed_generation{job="kube-state-metrics"}
              !=
            kube_statefulset_metadata_generation{job="kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
        - alert: KubeStatefulSetUpdateNotRolledOut
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
            summary: StatefulSet update has not been rolled out.
          expr: |
            (
              max without (revision) (
                kube_statefulset_status_current_revision{job="kube-state-metrics"}
                  unless
                kube_statefulset_status_update_revision{job="kube-state-metrics"}
              )
                *
              (
                kube_statefulset_replicas{job="kube-state-metrics"}
                  !=
                kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
              )
            )  and (
              changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[5m])
                ==
              0
            )
          for: 15m
          labels:
            severity: warning
        - alert: KubeDaemonSetRolloutStuck
          annotations:
            description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
            summary: DaemonSet rollout is stuck.
          expr: |
            (
              (
                kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              ) or (
                kube_daemonset_status_number_misscheduled{job="kube-state-metrics"}
                 !=
                0
              ) or (
                kube_daemonset_updated_number_scheduled{job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              ) or (
                kube_daemonset_status_number_available{job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              )
            ) and (
              changes(kube_daemonset_updated_number_scheduled{job="kube-state-metrics"}[5m])
                ==
              0
            )
          for: 15m
          labels:
            severity: warning
        - alert: KubeContainerWaiting
          annotations:
            description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
            summary: Pod container waiting longer than 1 hour
          expr: |
            sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics"}) > 0
          for: 1h
          labels:
            severity: warning
        - alert: KubeDaemonSetNotScheduled
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
            summary: DaemonSet pods are not scheduled.
          expr: |
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              -
            kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
          for: 10m
          labels:
            severity: warning
        - alert: KubeDaemonSetMisScheduled
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
            summary: DaemonSet pods are misscheduled.
          expr: |
            kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeJobCompletion
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than 12 hours to complete.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobcompletion
            summary: Job did not complete in time
          expr: |
            kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
          for: 12h
          labels:
            severity: warning
        - alert: KubeJobFailed
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
            summary: Job failed to complete.
          expr: |
            kube_job_failed{job="kube-state-metrics"}  > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeHpaReplicasMismatch
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch
            summary: HPA has not matched descired number of replicas.
          expr: |
            (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics"}
              !=
            kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"})
              and
            (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
              >
            kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics"})
              and
            (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
              <
            kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"})
              and
            changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}[15m]) == 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeHpaMaxedOut
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout
            summary: HPA is running at max replicas
          expr: |
            kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
              ==
            kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"}
          for: 15m
          labels:
            severity: warning
